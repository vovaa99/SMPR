# Линейные алгоритмы классификации
Пусть ![](https://latex.codecogs.com/gif.latex?X%20%3D%20%5Cmathbb%7BR%7D%5E%7Bn%7D%2C%20Y%20%3D%20%5Cleft%20%5C%7B%20-1%3B%20&plus;1%20%5Cright%20%5C%7D). **Линейным** называется следующий алгоритм классификации:

![](https://latex.codecogs.com/gif.latex?a%28x%2C%20w%29%20%3D%20sign%20%3Cw%2C%20x%3E) ,
где ![](https://latex.codecogs.com/gif.latex?w%20%5Cepsilon%20%5Cmathbb%7BR%7D%5En) -- вектор параметров.

Гиперплоскость, разделяющая классы в пространстве ![](https://latex.codecogs.com/gif.latex?%5Cmathbb%7BR%7D%5En), задаётся уравнением **<w, x> = 0**. Если вектор *x* находится по одну сторону гиперплоскости с её направляющим вектором *w*, то объект *x* относится к классу +1, иначе -- к классу -1.

Для того, чтобы подобрать оптимальный вектор параметров *w*, минимизирующий эмпирический риск ![](https://latex.codecogs.com/gif.latex?Q%28w%2C%20X%5El%29%20%3D%20%5Csum_%7Bi%20%3D%201%7D%5E%7Bl%7DL%28%3Cw%2C%20x_i%3Ey_i%29) ,

применяется **метод стохастического градиента**. В этом методе сначала выбирается начальное приближение для *w* (инициализируется небольшими случайными значениями *w = (-1/2n, 1/2n)*, где *n* -- число признаков *x*), затем запускается итерационный процесс, на каждом шаге которого вектор *w* сдвигается в направлении, противоположном направлению вектора градиента ![](https://latex.codecogs.com/gif.latex?Q%27%28w%2C%20X%5El%29).

Веса *w* изменяются следующим образом:

![](https://latex.codecogs.com/gif.latex?w%20%3D%20w%20-%20%5Ceta%20Q%27%28w%29), где ![](https://latex.codecogs.com/gif.latex?%5Ceta%20>%200) -- *темп обучения*. Для лучшей сходимости принято полагать ![](https://latex.codecogs.com/gif.latex?%5Ceta%20%3D%20%5Cfrac%7B1%7D%7Biteration%7D).

Функционал аппроксимированного эмпирического риска *Q* оценивается следующим образом:

![](https://latex.codecogs.com/gif.latex?Q%20%3D%20%281%20-%20%5Clambda%29Q%20&plus;%20%5Clambda*%5Cvarepsilon_i), где ![](https://latex.codecogs.com/gif.latex?%5Cvarepsilon_i%20%3D%20L%28<w%2C%20x_i>y_i%29) -- ошибка алгоритма на случайном объекте *x_i* из *Xl*, ![](https://latex.codecogs.com/gif.latex?%5Clambda) -- параметр сглаживания (обычно полагается *1/l*)

Сдвигаемся до тех пор, пока вектор *w* не перестанет изменяться и/или функционал *Q* не стабилизируется. Важно заметить, что градиент вычисляется не на всех объектах обучающей выборки, а на случайном объекте (поэтому *"стохастический"*). При использовании метода стохастического градиента нужно нормализовать данные:

![](https://latex.codecogs.com/gif.latex?x%5Ej%20%3D%20%5Cfrac%7Bx%5Ej%20-%20x_%7Bmean%7D%5Ej%7D%7Bx%5Ej_%7Bsd%7D%7D)

В зависимости от функции потерь *L* в функционале эмпирического риска *Q* существуют разнообразные **линейные алгоритмы классификации**.  